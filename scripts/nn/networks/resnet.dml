#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

source("scripts/nn/layers/batch_norm2d_old.dml") as bn2d
source("scripts/nn/layers/conv2d_builtin.dml") as conv2d
source("scripts/nn/layers/relu.dml") as relu
source("scripts/nn/layers/max_pool2d_builtin.dml") as mp2d
source("scripts/nn/layers/global_avg_pool2d.dml") as ap2d
source("scripts/nn/layers/affine.dml") as fc

conv3x3_forward = function(matrix[double] X, matrix[double] W,
                           int C_in, int C_out, int Hin, int Win,
                           int strideh, int stridew)
    return (matrix[double] out, int Hout, int Wout) {
    /*
     * Simple 2D conv layer with 3x3 filter
     */
    # bias should not be applied
    bias = matrix(0, C_out, 1)
    [out, Hout, Wout] = conv2d::forward(X, W, bias, C_in, Hin, Win, 3, 3, strideh, stridew, 1, 1)
}

conv3x3_backward = function(matrix[double] dOut, int Hout, int Wout,
                            matrix[double] X, matrix[double] W,
                            int C_in, int C_out, int Hin, int Win,
                            int strideh, int stridew)
    return (matrix[double] dX, matrix[double] dW) {
    /*
     * Simple 2D conv layer with 3x3 filter
     */
    bias = matrix(0, C_out, 1)
    [dX, dW, db] = conv2d::backward(dOut, Hout, Wout, X, W, bias, C_in, Hin, Win, 3, 3, strideh, stridew, 1, 1)
}

conv1x1_forward = function(matrix[double] X, matrix[double] W,
                           int C_in, int C_out, int Hin, int Win,
                           int strideh, int stridew)
    return(matrix[double] out, int Hout, int Wout) {
    /*
     * Simple 2D conv layer with 1x1 filter
     */
    # bias should not be applied
    bias = matrix(0, C_out, 1)
    [out, Hout, Wout] = conv2d::forward(X, W, bias, C_in, Hin, Win, 1, 1, strideh, stridew, 0, 0)
}

conv1x1_backward = function(matrix[double] dOut, int Hout, int Wout,
                           matrix[double] X, matrix[double] W,
                           int C_in, int C_out, int Hin, int Win,
                           int strideh, int stridew)
    return(matrix[double] dX, matrix[double] dW) {
    /*
     * Simple 2D conv layer with 1x1 filter
     */
    bias = matrix(0, C_out, 1)
    [dX, dW, db] = conv2d::backward(dOut, Hout, Wout, X, W, bias, C_in, Hin, Win, 1, 1, strideh, stridew, 0, 0)
}

basic_block_forward = function(matrix[double] X, list[unknown] weights,
                             int C_in, int C_base, int Hin, int Win,
                             int strideh, int stridew, string mode,
                             list[unknown] ema_means_vars)
    return (matrix[double] out, int Hout, int Wout,
            list[unknown] ema_means_vars_upd,
            list[unknown] cached_out,
            list[unknown] cached_means_vars) {
    /*
     * Computes the forward pass for a basic residual block.
     * This basic residual block (with 2 3x3 conv layers of
     * same channel size) is used in the smaller ResNets 18
     * and 34.
     *
     * Inputs:
     * - X: Inputs, of shape (N, C_in*Hin*Win).
     * - weights: list of weights for all layers of res block
     *     with the following order/content:
     *   -> 1: Weights of conv 1, of shape (C_base, C_in*3*3).
     *   -> 2: Weights of batch norm 1, of shape (C_base, 1).
     *   -> 3: Bias of batch norm 1, of shape (C_base, 1).
     *   -> 4: Weights of conv 2, of shape (C_base, C_base*3*3).
     *   -> 5: Weights of batch norm 2, of shape (C_base, 1).
     *   -> 6: Bias of batch norm 2, of shape (C_base, 1).
     *   If the block should downsample X:
     *   -> 7: Weights of downsample conv, of shape (C_base, C_in*1*1).
     *   -> 8: Weights of downsample batch norm, of shape (C_base, 1).
     *   -> 9: Bias of downsample batch norm, of shape (C_base, 1).
     * - C_in: Number of input channels.
     * - C_base: Number of base channels for this block.
     * - Hin: Input height.
     * - Win: Input width.
     * - strideh: Stride over height (usually 1 or 2).
     * - stridew: Stride over width (usually same as strideh).
     * - mode: 'train' or 'test' to indicate if the model is currently
     *     being trained or tested for badge normalization layers.
     *     See badge_norm2d.dml docs for more info.
     * - ema_means_vars: List of exponential moving averages for mean
     *     and variance for badge normalization layers.
     *   -> 1: EMA for mean of badge norm 1, of shape (C_base, 1).
     *   -> 2: EMA for variance of badge norm 1, of shape (C_base, 1).
     *   -> 3: EMA for mean of badge norm 2, of shape (C_base, 1).
     *   -> 4: EMA for variance of badge norm 2, of shape (C_base, 1).
     *   If the block should downsample X:
     *   -> 5: EMA for mean of downs. badge norm, of shape (C_base, 1).
     *   -> 6: EMA for variance of downs. badge norm, of shape (C_base, 1).
     *
     * Outputs:
     * - out: Output, of shape (N, C_base*Hout*Wout).
     * - Hout: Output height.
     * - Wout: Output width.
     * - ema_means_vars_upd: List of updated exponential moving averages
     *     for mean and variance of badge normalization layers.
     * - cached_out: Outputs of each layer for computation of backward
     *     pass. Refer to the code for the order of elements.
     * - cached_means_vars: List of cached means and vars returned from
     *     each batch normalization layer. This is required for the
     *     backward pass of the network.
     */
    downsample = strideh > 1 | stridew > 1 | C_in != C_base
    # default values
    mu_bn = 0.1
    epsilon_bn = 1e-05

    # get all params
    W_conv1 = as.matrix(weights[1])
    gamma_bn1 = as.matrix(weights[2])
    beta_bn1 = as.matrix(weights[3])
    W_conv2 = as.matrix(weights[4])
    gamma_bn2 = as.matrix(weights[5])
    beta_bn2 = as.matrix(weights[6])

    ema_mean_bn1 = as.matrix(ema_means_vars[1])
    ema_var_bn1 = as.matrix(ema_means_vars[2])
    ema_mean_bn2 = as.matrix(ema_means_vars[3])
    ema_var_bn2 = as.matrix(ema_means_vars[4])

    if (downsample) {
        # gather params for downsampling
        W_conv3 = as.matrix(weights[7])
        gamma_bn3 = as.matrix(weights[8])
        beta_bn3 = as.matrix(weights[9])
        ema_mean_bn3 = as.matrix(ema_means_vars[5])
        ema_var_bn3 = as.matrix(ema_means_vars[6])
    }

    # RESIDUAL PATH
    # First convolutional layer
    [out_conv1, Hout, Wout] = conv3x3_forward(X, W_conv1, C_in, C_base, Hin, Win,
                                              strideh, stridew)
    [out_bn1, ema_mean_bn1_upd, ema_var_bn1_upd, c_m_bn1, c_v_bn1] = bn2d::forward(out_conv1, gamma_bn1, beta_bn1, C_base, Hout,
                                                             Wout, mode, ema_mean_bn1, ema_var_bn1, mu_bn, epsilon_bn)
    out_re1 = relu::forward(out_bn1)

    # Second convolutional layer
    [out_conv2, Hout, Wout] = conv3x3_forward(out_re1, W_conv2, C_base, C_base, Hout, Wout, 1, 1)
    [out_bn2, ema_mean_bn2_upd, ema_var_bn2_upd, c_m_bn2, c_v_bn2] = bn2d::forward(out_conv2, gamma_bn2, beta_bn2, C_base, Hout,
                                                             Wout, mode, ema_mean_bn2, ema_var_bn2, mu_bn, epsilon_bn)

    # IDENTITY PATH
    if (downsample) {
        # downsample input
        [out_conv3, Hout, Wout] = conv1x1_forward(X, W_conv3, C_in, C_base, Hin, Win, strideh, stridew)
        [out_bn3, ema_mean_bn3_upd, ema_var_bn3_upd, c_m_bn3, c_v_bn3] = bn2d::forward(out_conv3, gamma_bn3, beta_bn3,
                                               C_base, Hout, Wout, mode, ema_mean_bn3, ema_var_bn3, mu_bn, epsilon_bn)
        identity = out_bn3
    } else {
        identity = X
    }

    out_re2 = relu::forward(out_bn2 + identity)

    ema_means_vars_upd = list(ema_mean_bn1_upd, ema_var_bn1_upd, ema_mean_bn2_upd, ema_var_bn2_upd)
    cached_out = list(out_conv1, out_bn1, out_re1, out_conv2, out_bn2, out_re2)
    cached_means_vars = list(c_m_bn1, c_v_bn1, c_m_bn2, c_v_bn2)
    if (downsample) {
        ema_means_vars_upd = append(ema_means_vars_upd, ema_mean_bn3_upd)
        ema_means_vars_upd = append(ema_means_vars_upd, ema_var_bn3_upd)
        cached_out = append(cached_out, out_conv3)
        cached_out = append(cached_out, out_bn3)
        cached_means_vars = append(cached_means_vars, c_m_bn3)
        cached_means_vars = append(cached_means_vars, c_v_bn3)
    }

    out = out_re2
}

/*
basic_block_backward = function(matrix[double] dOut, int Hout, int Wout,
                                list[unknown] cached_out, matrix[double] X,
                                list[unknown] weights, int C_in, int C_base,
                                int Hin, int Win, int strideh, int stridew,
                                string mode, list[unknown] cached_means_vars)
    return (list[unknown] gradients) {
    downsample = strideh > 1 | stridew > 1 | C_in != C_base
    # default values
    mu_bn = 0.1
    epsilon_bn = 1e-05

    # get all params
    W_conv1 = as.matrix(weights[1])
    gamma_bn1 = as.matrix(weights[2])
    beta_bn1 = as.matrix(weights[3])
    W_conv2 = as.matrix(weights[4])
    gamma_bn2 = as.matrix(weights[5])
    beta_bn2 = as.matrix(weights[6])

    out_conv1 = as.matrix(cached_out[1])
    out_bn1 = as.matrix(cached_out[2])
    out_re1 = as.matrix(cached_out[3])
    out_conv2 = as.matrix(cached_out[4])
    out_bn2 = as.matrix(cached_out[5])
    out_re2 = as.matrix(cached_out[6])

    mean_bn1 = as.matrix(cached_means_vars[1])
    var_bn1 = as.matrix(cached_means_vars[2])
    mean_bn2 = as.matrix(cached_means_vars[3])
    var_bn2 = as.matrix(cached_means_vars[4])

    if (downsample) {
        # gather params for downsampling
        W_conv3 = as.matrix(weights[7])
        gamma_bn3 = as.matrix(weights[8])
        beta_bn3 = as.matrix(weights[9])

        out_conv3 = as.matrix(cached_out[7])
        out_bn3 = as.matrix(cached_out[8])
        identity = out_bn3

        mean_bn3 = as.matrix(cached_means_vars[5])
        var_bn3 = as.matrix(cached_means_vars[6])
    } else {
        identity = X
    }

    dX = relu::backward(dOut, identity + out_bn2)

    # IDENTITY PATH
    if (downsample) {
        [dX_id, dgamma_bn3, dbeta_bn3] = bn2d::backward(dX, mean_bn3, var_bn3, out_conv3, gamma_bn3, C_base, Hout, Wout,
            epsilon_bn)
        [dX_id, dW_conv3, db_conv3] = conv3x3_backward(dX_id, Hout, Wout, X, W_conv3,
    }

}
 */

bottleneck_block_forward = function(matrix[double] X,
        list[unknown] weights, int C_in, int C_base, int Hin,
        int Win, int strideh, int stridew, string mode,
        list[unknown] ema_means_vars)
    return (matrix[double] out, int Hout, int Wout,
            list[unknown] ema_means_vars_upd,
            list[unknown] cached_out,
            list[unknown] cached_means_vars) {
    /*
     * Computes the forward pass for a bottleneck residual
     * block.
     * This residual block architecture is used in the
     * bigger ResNets. They consist of 3 convolutional
     * layers - 1x1, 3x3, 1x1. The last layer increases
     * the number of channels by a factor of 4 which is
     * downscaled by the first layer of the next res block
     * to always keep computational complexity at a
     * minimum.
     * The downsampling of the image dimensions (Hin & Win)
     * through the stride is placed at the 3x3 conv layer
     * instead of the first 1x1 layer (as proposed in "Deep
     * residual learning for image recognition") which is
     * called ResNet V1.5 and introduced in
     * https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.
     *
     * Inputs:
     * - X: Inputs, of shape (N, C_in*Hin*Win).
     * - weights: list of weights for all layers of res block
     *     with the following order/content:
     *   -> 1: Weights of conv 1, of shape (C_base, C_in*1*1).
     *   -> 2: Weights of batch norm 1, of shape (C_base, 1).
     *   -> 3: Bias of batch norm 1, of shape (C_base, 1).
     *   -> 4: Weights of conv 2, of shape (C_base, C_base*3*3).
     *   -> 5: Weights of batch norm 2, of shape (C_base, 1).
     *   -> 6: Bias of batch norm 2, of shape (C_base, 1).
     *   -> 7: Weights of conv 3, of shape (4*C_base, C_base*1*1).
     *   -> 8: Weights of batch norm 3, of shape (4*C_base, 1).
     *   -> 9: Bias of batch norm 3, of shape (4*C_base, 1).
     *   If the block should downsample X:
     *   -> 10: Weights of downsample conv, of shape (4*C_base, C_in*1*1).
     *   -> 11: Weights of downsample batch norm, of shape (4*C_base, 1).
     *   -> 12: Bias of downsample batch norm, of shape (4*C_base, 1).
     * - C_in: Number of input channels.
     * - C_base: Number of base channels for this block.
     * - Hin: Input height.
     * - Win: Input width.
     * - strideh: Stride over height (usually 1 or 2).
     * - stridew: Stride over width (usually same as strideh).
     * - mode: 'train' or 'test' to indicate if the model is currently
     *     being trained or tested for badge normalization layers.
     *     See badge_norm2d.dml docs for more info.
     * - ema_means_vars: List of exponential moving averages for mean
     *     and variance for badge normalization layers.
     *   -> 1: EMA for mean of badge norm 1, of shape (C_base, 1).
     *   -> 2: EMA for variance of badge norm 1, of shape (C_base, 1).
     *   -> 3: EMA for mean of badge norm 2, of shape (C_base, 1).
     *   -> 4: EMA for variance of badge norm 2, of shape (C_base, 1).
     *   -> 5: EMA for mean of badge norm 3, of shape (4*C_base, 1).
     *   -> 6: EMA for variance of badge norm 3, of shape (4*C_base, 1).
     *   If the block should downsample X:
     *   -> 7: EMA for mean of downs. badge norm, of shape (4*C_base, 1).
     *   -> 8: EMA for variance of downs. badge norm, of shape (4*C_base, 1).
     *
     * Outputs:
     * - out: Output, of shape (N, 4*C_base*Hout*Wout).
     * - Hout: Output height.
     * - Wout: Output width.
     * - ema_means_vars_upd: List of updated exponential moving averages
     *     for mean and variance of badge normalization layers.
     * - cached_out: Outputs of each layer for computation of backward
     *     pass. Refer to the code for the order of elements.
     * - cached_means_vars: List of cached means and vars returned from
     *     each batch normalization layer. This is required for the
     *     backward pass of the network.
     */
    downsample = strideh > 1 | stridew > 1 | C_in != 4 * C_base
    # default values
    mu_bn = 0.1
    epsilon_bn = 1e-05

    # get all params
    W_conv1 = as.matrix(weights[1])
    gamma_bn1 = as.matrix(weights[2])
    beta_bn1 = as.matrix(weights[3])
    W_conv2 = as.matrix(weights[4])
    gamma_bn2 = as.matrix(weights[5])
    beta_bn2 = as.matrix(weights[6])
    W_conv3 = as.matrix(weights[7])
    gamma_bn3 = as.matrix(weights[8])
    beta_bn3 = as.matrix(weights[9])

    ema_mean_bn1 = as.matrix(ema_means_vars[1])
    ema_var_bn1 = as.matrix(ema_means_vars[2])
    ema_mean_bn2 = as.matrix(ema_means_vars[3])
    ema_var_bn2 = as.matrix(ema_means_vars[4])
    ema_mean_bn3 = as.matrix(ema_means_vars[5])
    ema_var_bn3 = as.matrix(ema_means_vars[6])

    if (downsample) {
        # gather params for donwsampling
        W_conv4 = as.matrix(weights[10])
        gamma_bn4 = as.matrix(weights[11])
        beta_bn4 = as.matrix(weights[12])
        ema_mean_bn4 = as.matrix(ema_means_vars[7])
        ema_var_bn4 = as.matrix(ema_means_vars[8])
    }

    # RESIDUAL PATH
    # First convolutional layer
    [out_conv1, Hout, Wout] = conv1x1_forward(X, W_conv1, C_in, C_base, Hin, Win,
                                        1, 1)
    [out_bn1, ema_mean_bn1_upd, ema_var_bn1_upd, c_m_bn1, c_v_bn1] = bn2d::forward(out_conv1, gamma_bn1, beta_bn1, C_base, Hout,
                                                             Wout, mode, ema_mean_bn1, ema_var_bn1, mu_bn, epsilon_bn)
    out_re1 = relu::forward(out_bn1)

    # Second convolutional layer
    [out_conv2, Hout, Wout] = conv3x3_forward(out_re1, W_conv2, C_base, C_base, Hout, Wout, strideh, stridew)
    [out_bn2, ema_mean_bn2_upd, ema_var_bn2_upd, c_m_bn2, c_v_bn2] = bn2d::forward(out_conv2, gamma_bn2, beta_bn2, C_base, Hout,
                                                             Wout, mode, ema_mean_bn2, ema_var_bn2, mu_bn, epsilon_bn)
    out_re2 = relu::forward(out_bn2)

    # Third convolutional layer
    [out_conv3, Hout, Wout] = conv1x1_forward(out_re2, W_conv3, C_base, 4*C_base, Hout, Wout, 1, 1)
    [out_bn3, ema_mean_bn3_upd, ema_var_bn3_upd, c_m_bn3, c_v_bn3] = bn2d::forward(out_conv3, gamma_bn3, beta_bn3, 4*C_base, Hout,
                                                               Wout, mode, ema_mean_bn3, ema_var_bn3, mu_bn, epsilon_bn)

    # IDENTITY PATH
    if (downsample) {
        # downsample input
        [out_conv4, Hout, Wout] = conv1x1_forward(X, W_conv4, C_in, 4*C_base,
                                                 Hin, Win, strideh, stridew)
        [out_bn4, ema_mean_bn4_upd, ema_var_bn4_upd, c_m_bn4, c_v_bn4] = bn2d::forward(out_conv4, gamma_bn4, beta_bn4,
                                             4*C_base, Hout, Wout, mode, ema_mean_bn4, ema_var_bn4, mu_bn, epsilon_bn)
        identity = out_bn4
    } else {
        identity = X
    }

    out_re3 = relu::forward(out_bn3 + identity)

    ema_means_vars_upd = list(ema_mean_bn1_upd, ema_var_bn1_upd, ema_mean_bn2_upd, ema_var_bn2_upd, ema_mean_bn3_upd,
                              ema_var_bn3_upd)
    cached_out = list(out_conv1, out_bn1, out_re1, out_conv2, out_bn2, out_re2, out_conv3, out_bn3, out_re3)
    cached_means_vars = list(c_m_bn1, c_v_bn1, c_m_bn2, c_v_bn2, c_m_bn3, c_v_bn3)
    if (downsample) {
        ema_means_vars_upd = append(ema_means_vars_upd, ema_mean_bn3_upd)
        ema_means_vars_upd = append(ema_means_vars_upd, ema_var_bn3_upd)
        cached_out = append(cached_out, out_conv4)
        cached_out = append(cached_out, out_bn4)
        cached_means_vars = append(cached_means_vars, c_m_bn4)
        cached_means_vars = append(cached_means_vars, c_v_bn4)
    }

    out = out_re3
}

reslayer_forward = function(matrix[double] X, int Hin, int Win,
                            string block_type, int blocks, int strideh,
                            int stridew, int C_in, int C_base,
                            list[unknown] blocks_weights, string mode,
                            list[unknown] ema_means_vars)
    return (matrix[double] out, int Hout, int Wout,
            list[unknown] ema_means_vars_upd,
            list[unknown] cached_out,
            list[unknown] cached_means_vars) {
    /*
     * Executes the forward pass for a sequence of residual
     * blocks with the same number of base channels, i.e.
     * a residual layer.
     *
     * Inputs:
     * - X: Inputs, of shape (N, C_in*Hin*Win)
     * - Hin: Input height.
     * - Win: Input width.
     * - block_type: 'basic' or 'bottleneck' depending on
     *     which type of block should be used for the
     *     residual layer.
     * - blocks: Number of residual blocks (bigger than 0).
     * - strideh: Stride height for first conv layer of first block.
     * - stridew: Stride width for first conv layer of first block.
     * - C_in: Number of input channels.
     * - C_base: Number of base channels of res layer.
     * - blocks_weights: List of weights of each block.
     *     -> i: List of weights of block i with the content
     *           defined in the docs of basic_block_forward()
     *           or bottleneck_block_forward() depending on
     *           the block type.
     *     -> length == blocks
     * - mode: 'train' or 'test' to indicate if the model is currently
     *     being trained or tested for badge normalization layers.
     *     See badge_norm2d.dml docs for more info.
     * - ema_means_vars: List of exponential moving averages for mean
     *     and variance for badge normalization layers of each block.
     *     -> i: List of EMAs of block i with the content defined
     *           in the docs of basic_block_forward() or
     *           bottleneck_block_forward() depending on the block type.
     *     -> length == blocks
     * - cached_out: Outputs of each layer for computation of backward
     *     pass. Refer to the code for the order of elements.
     * - cached_means_vars: List of cached means and vars returned from
     *     each batch normalization layer. This is required for the
     *     backward pass of the network.
     */
    # default values
    mu_bn = 0.1
    epsilon_bn = 1e-05

    # first block with provided stride
    if (block_type == "basic") {
        [out, Hout, Wout, emas1_upd, cached_out_b1, cached_mv_b1] = basic_block_forward(X, as.list(blocks_weights[1]), C_in, C_base,
                                                        Hin, Win, strideh, stridew, mode, as.list(ema_means_vars[1]))
    } else {
        [out, Hout, Wout, emas1_upd, cached_out_b1, cached_mv_b1] = bottleneck_block_forward(X, as.list(blocks_weights[1]), C_in,
                                             C_base, Hin, Win, strideh, stridew, mode, as.list(ema_means_vars[1]))
    }
    ema_means_vars_upd = list(emas1_upd)
    cached_out = list(cached_out_b1)
    cached_means_vars = list(cached_mv_b1)

    # other blocks
    for (i in 2:blocks) {
        current_weights = as.list(blocks_weights[i])
        current_emas = as.list(ema_means_vars[i])
        if (block_type == "basic") {
            [out, Hout, Wout, current_emas_upd, current_cached_out, current_cached_mv] = basic_block_forward(X=out, weights=current_weights,
                                        C_in=C_base, C_base=C_base, Hin=Hout, Win=Wout, strideh=1, stridew=1, mode=mode,
                                        ema_means_vars=current_emas)
        } else {
            [out, Hout, Wout, current_emas_upd, current_cached_out, current_cached_mv] = bottleneck_block_forward(X=out,
                                        weights=current_weights, C_in=C_base*4, C_base=C_base, Hin=Hout, Win=Wout,
                                        strideh=1, stridew=1, mode=mode, ema_means_vars=current_emas)
        }
        ema_means_vars_upd = append(ema_means_vars_upd, current_emas_upd)
        cached_out = append(cached_out, current_cached_out)
        cached_means_vars = append(cached_means_vars, current_cached_mv)
    }
}

resnet_forward = function(matrix[double] X, int Hin, int Win,
                          string block_type, list[unknown] layer_sizes,
                          list[unknown] model, string mode,
                          list[unknown] ema_means_vars)
    return (matrix[double] out, list[unknown] ema_means_vars_upd,
            list[unknown] cached_out, list[unknown] cached_means_vars) {
    /*
     * Forward pass of the ResNet as introduced in
     * "Deep Residual Learning for Image Recognition" by
     * Kaiming He et. al. and inspired by the PyTorch
     * implementation.
     *
     * Inputs:
     * - X: Inputs, of shape (N, C_in*Hin*Win).
     *     C_in = 3 is expected.
     * - Hin: Input height.
     * - Win: Input width.
     * - block_type: 'basic' or 'bottleneck' depending on
     *     which type of block should be used for the
     *     residual network.
     * - layer_sizes: List of the sizes of each of
     *     the 4 residual layers.
     *     For ResNet18: [2, 2, 2, 2], RN34: [3, 4, 6, 3],
     *     RN50: [3, 4, 6, 3], RN101: [3, 4, 23, 3],
     *     RN152: [3, 8, 36, 3]
     * - model: Weights and bias matrices of the model
     *     with the following order/content:
     *   -> 1: Weights of conv 1 7x7, of shape (64, 3*7*7)
     *   -> 2: Weights of batch norm 1, of shape (64, 1).
     *   -> 3: Bias of batch norm 1, of shape (64, 1).
     *   -> 4: List of weights for first residual layer
     *         with 64 base channels.
     *   -> 5: List of weights for second residual layer
     *         with 128 base channels.
     *   -> 6: List of weights for third residual layer
     *         with 256 base channels.
     *   -> 7: List of weights for fourth residual layer
     *         with 512 base channels.
     *      List of residual layers 1, 2, 3 & 4 have
     *      the content/order:
     *      -> i: List of weights for residual block i.
     *            with i in {1, ..., layer_sizes[layer]}
     *         Each list of weights for a residual block
     *         must follow the same order as defined in
     *         the documentation of basic_block_forward()
     *         or bottleneck_block_forward() depending
     *         on the block type.
     *   -> 8: Weights of fully connected layer, of shape (C_out, 1000)
     *         where C_out = 512 for basic block type and C_out = 2048
     *         for bottleneck block type.
     *   -> 9: Bias of fully connected layer, of shape (1, 1000)
     * - mode: 'train' or 'test' to indicate if the model is currently
     *     being trained or tested for badge normalization layers.
     *     See badge_norm2d.dml docs for more info.
     * - ema_means_vars: List of exponential moving averages for mean
     *     and variance for badge normalization layers.
     *   -> 1: EMA for mean of badge norm 1, of shape (64, 1).
     *   -> 2: EMA for variance of badge norm 1, of shape (64, 1).
     *   -> 3: List of EMA means and vars for residual layer 1.
     *   -> 4: List of EMA means and vars for residual layer 2.
     *   -> 5: List of EMA means and vars for residual layer 3.
     *   -> 6: List of EMA means and vars for residual layer 4.
     *      Lists for EMAs of layer 1, 2, 3 & 4 must have the
     *      following order:
     *      -> i: List of EMA means and vars for residual block i.
     *            with i in {1, ..., layer_sizes[layer]}
     *         Each list of EMAs for a residual block
     *         must follow the same order as defined in
     *         the documentation of basic_block_forward()
     *         or bottleneck_block_forward().
     * - NOTICE: The lists of the first blocks for layer 2, 3 and 4
     *           must include weights and EMAs for 1 extra conv layer
     *           and a batch norm layer for the downsampling on the
     *           identity path.
     *
     * Outputs:
     * - out: Outputs, of shape (N, 1000)
     * - ema_means_vars_upd: List of updated exponential moving averages
     *     for mean and variance of badge normalization layers. It follows
     *     the same exact structure as the input EMAs list.
     * - cached_out: Outputs of each layer for computation of backward
     *     pass. Refer to the code for the order of elements.
     * - cached_means_vars: List of cached means and vars returned from
     *     each batch normalization layer. This is required for the
     *     backward pass of the network.
     */
    # default values
    mu_bn = 0.1
    epsilon_bn = 1e-05

    if (block_type == "basic") {
        block_expansion = 1
    } else {
        block_expansion = 4
    }

    # extract model params
    W_conv1 = as.matrix(model[1])
    gamma_bn1 = as.matrix(model[2]); beta_bn1 = as.matrix(model[3])
    weights_reslayer1 = as.list(model[4])
    weights_reslayer2 = as.list(model[5])
    weights_reslayer3 = as.list(model[6])
    weights_reslayer4 = as.list(model[7])
    W_fc = as.matrix(model[8])
    b_fc = as.matrix(model[9])
    ema_mean_bn1 = as.matrix(ema_means_vars[1]); ema_var_bn1 = as.matrix(ema_means_vars[2])
    emas_reslayer1 = as.list(ema_means_vars[3])
    emas_reslayer2 = as.list(ema_means_vars[4])
    emas_reslayer3 = as.list(ema_means_vars[5])
    emas_reslayer4 = as.list(ema_means_vars[6])

    # Convolutional 7x7 layer
    C = 64
    b_conv1 = matrix(0, rows=C, cols=1)
    [out_conv1, Hout, Wout] = conv2d::forward(X=X, W=W_conv1, b=b_conv1, C=3, Hin=Hin, Win=Win, Hf=7, Wf=7, strideh=2,
                                        stridew=2, padh=3, padw=3)
    # Batch Normalization
    [out_bn1, ema_mean_bn1_upd, ema_var_bn1_upd, cached_m, cached_v] = bn2d::forward(X=out_conv1, gamma=gamma_bn1, beta=beta_bn1,
        C=C, Hin=Hout, Win=Wout, mode=mode, ema_mean=ema_mean_bn1, ema_var=ema_var_bn1, mu=mu_bn, epsilon=epsilon_bn)
    # ReLU
    out_re1 = relu::forward(X=out_bn1)
    # Max Pooling 3x3
    [out_mp, Hout, Wout] = mp2d::forward(X=out_re1, C=C, Hin=Hout, Win=Wout, Hf=3, Wf=3, strideh=2, stridew=2, padh=1, padw=1)

    # residual layer 1
    block_count = as.integer(as.scalar(layer_sizes[1]))
    [out, Hout, Wout, emas1_upd, cached_out_l1, cached_mv_l1] = reslayer_forward(X=out_mp, Hin=Hout, Win=Wout, block_type=block_type,
        blocks=block_count, strideh=1, stridew=1, C_in=C, C_base=64, blocks_weights=weights_reslayer1, mode=mode,
        ema_means_vars=emas_reslayer1)
    C = 64 * block_expansion
    # residual layer 2
    block_count = as.integer(as.scalar(layer_sizes[2]))
    [out, Hout, Wout, emas2_upd, cached_out_l2, cached_mv_l2] = reslayer_forward(X=out, Hin=Hout, Win=Wout, block_type=block_type,
        blocks=block_count, strideh=2, stridew=2, C_in=C, C_base=128, blocks_weights=weights_reslayer2, mode=mode,
        ema_means_vars=emas_reslayer2)
    C = 128 * block_expansion
    # residual layer 3
    block_count = as.integer(as.scalar(layer_sizes[3]))
    [out, Hout, Wout, emas3_upd, cached_out_l3, cached_mv_l3] = reslayer_forward(X=out, Hin=Hout, Win=Wout, block_type=block_type,
        blocks=block_count, strideh=2, stridew=2, C_in=C, C_base=256, blocks_weights=weights_reslayer3, mode=mode,
        ema_means_vars=emas_reslayer3)
    C = 256 * block_expansion
    # residual layer 4
    block_count = as.integer(as.scalar(layer_sizes[4]))
    [out, Hout, Wout, emas4_upd, cached_out_l4, cached_mv_l4] = reslayer_forward(X=out, Hin=Hout, Win=Wout, block_type=block_type,
        blocks=block_count, strideh=2, stridew=2, C_in=C, C_base=512, blocks_weights=weights_reslayer4, mode=mode,
        ema_means_vars=emas_reslayer4)
    C = 512 * block_expansion

    # Global Average Pooling
    [out_ap, Hout, Wout] = ap2d::forward(X=out, C=C, Hin=Hout, Win=Wout)
    # Affine
    out_fc = fc::forward(X=out_ap, W=W_fc, b=b_fc)

    ema_means_vars_upd = list(ema_mean_bn1_upd, ema_var_bn1_upd, emas1_upd, emas2_upd, emas3_upd, emas4_upd)
    cached_out = list(out_conv1, out_bn1, out_re1, out_mp, cached_out_l1, cached_out_l2, cached_out_l3, cached_out_l4, out_ap, out_fc)
    cached_means_vars = list(cached_m, cached_v, cached_mv_l1, cached_mv_l2, cached_mv_l3, cached_mv_l4)

    out = out_fc
}
