#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

#-------------------------------------------------------------
# This is a simple example for the use of the ResNets. Here,
# ResNet18 is used. Random data is used to only showcase the
# usage of the ResNets.
#-------------------------------------------------------------

source("scripts/nn/networks/resnet.dml") as resnet
source("scripts/nn/networks/resnet_util.dml") as resnet_util
source("scripts/nn/networks/resnet18.dml") as resnet18
source("scripts/nn/layers/softmax.dml") as softmax
source("scripts/nn/layers/cross_entropy_loss.dml") as cross_entropy
source("scripts/nn/layers/logcosh_loss.dml") as logcosh
source("nn/optim/adam.dml") as adam

# resnet18
reslayer_sizes = list(2, 2, 2, 2)
block_type = "basic"

# get initial model parameters
[model, ema_means_vars] = resnet::init(block_type, reslayer_sizes, -1)

# get initial optimizer parameters
optimizer_params = resnet_util::init_optim("adam", block_type, reslayer_sizes)

# create random data
N = 100
Hin = 32
Win = 32
C = 3  # input channels
F = 1000  # output classes
X = rand(rows=N, cols=Hin*Win*C)
Y = rand(rows=N, cols=F, min=0, max=1, pdf="normal")

# train ResNet
epochs = 20
batch_size = 16
# adam optimizer params
lr = 0.001
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8
t = 0

[learned_model, learned_emas] = train(X, Y, model, ema_means_vars, N, Hin, Win, epochs, batch_size, optimizer_params,
    lr, beta1, beta2, epsilon, t)


train = function(matrix[double] X, matrix[double] Y, list[unknown] model, list[unknown] emas, int samples, int Hin,
    int Win, int epochs, int batch_size, list[unknown] optim_params, double lr, double beta1, double beta2,
    double epsilon, int t)
    return (list[unknown] learned_model, list[unknown] learned_emas) {

    iterations = ceil(samples/batch_size)
    mode = "train"

    for (epoch in 1:epochs) {
        loss_avg = 0.0

        print("Start epoch: " + epoch)

        for (i in 1:iterations) {
            print(" - Iteration: " + i)

            # get batch
            start = (i - 1) * batch_size + 1
            end = min(samples, i * batch_size)
            X_batch = X[start:end,]
            Y_batch = Y[start:end,]

            # forward pass
            [out, emas, cached_out, cached_means_vars] = resnet18::forward(X_batch, Hin, Win, model, mode, emas)

            # loss
            loss = logcosh::forward(out, Y_batch)
            loss_avg = (loss_avg * (i - 1) + loss) / i

            # backward
            dOut = logcosh::backward(out, Y_batch)
            [dX, gradients] = resnet18::backward(dOut, cached_out, model, cached_means_vars)

            # update parameters
            optim_hyper_params = list(lr, beta1, beta2, epsilon, t)
            # optim_hyper_params = list(lr, epsilon)
            # optim_hyper_params = list(lr, 0.99, epsilon)
            # sgd nesterov & momentum
            # optim_hyper_params = list(lr, 0.8)
            # sgd
            # optim_hyper_params = list(lr)
            [optim_params, model] = resnet_util::update_params("adam", optim_params, optim_hyper_params, gradients,
                model, "basic", list(2, 2, 2, 2))

            t = t + 1
        }

        # reshuffle mini batches
        r = rand(rows=nrow(Y), cols=1, min=0, max=1, pdf="uniform")
        X_tmp = order(target=cbind(r, X), by=1)
        Y_tmp = order(target=cbind(r, Y), by=1)
        X = X_tmp[,2:ncol(X_tmp)]
        Y = Y_tmp[,2:ncol(Y_tmp)]

        print("Loss: " + loss_avg)
    }

    learned_model = list()
    learned_emas = list()
}
